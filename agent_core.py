import asyncio
from typing import AsyncIterable, Any

from langchain.agents import AgentExecutor, create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.tools import tool, render_text_description
from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.pydantic_v1 import BaseModel,Field
from langchain.callbacks.base import AsyncCallbackHandler

from dotenv import load_dotenv
from tools.tool1_å¤©æ°”æŸ¥è¯¢ import get_weather
from tools.tool2_æ—¶é—´è·å– import get_current_time
from tools.tool4_finance import get_stock_data

load_dotenv()

# æ­¤å¤„çš„å¤§æ¨¡å‹ä¸æ˜¯ç®€å•çš„äº‘ç«¯è°ƒç”¨
# è€Œæ˜¯éœ€è¦å°è£…å·¥å…·ï¼Œä¹Ÿå°±æ˜¯ Agent = LLM + Tools
# REACT Reasoning-->Action-->Observation
# æ³¨å†Œå·¥å…·
class StockInput(BaseModel):
    ticker: str = Field(
        description="è‚¡ç¥¨ä»£ç ã€‚ç¾è‚¡ç›´æ¥ç”¨ä»£ç (å¦‚AAPL)ï¼ŒAè‚¡éœ€åŠ åç¼€(å¦‚600519.SSè¡¨ç¤ºèŒ…å°)ï¼Œæ¸¯è‚¡åŠ åç¼€(å¦‚00700.HK)ã€‚")
class SearchInput(BaseModel):
    query: str = Field(description="æœç´¢å¼•æ“çš„æŸ¥è¯¢å…³é”®è¯")
@tool("stock_tool",args_schema=StockInput)
def stock_tool(ticker:str):
    """
    æŸ¥è¯¢è‚¡ç¥¨çš„å®æ—¶ä»·æ ¼ã€å¸‚å€¼ã€PEç­‰åŸºæœ¬é¢æ•°æ®ã€‚
    å¦‚æœä¸çŸ¥é“ä»£ç ï¼Œè¯·å…ˆä½¿ç”¨ search_tool æŸ¥è¯¢ã€‚
    """
    return get_stock_data(ticker)

search = DuckDuckGoSearchRun()
@tool("search_tool", args_schema=SearchInput)
def search_tool(query: str):
    """
    ç”¨äºæœç´¢ï¼š
    1. è‚¡ç¥¨ä»£ç ï¼ˆå¦‚'èŒ…å° è‚¡ç¥¨ä»£ç 'ï¼‰
    2. è¿‘æœŸè´¢ç»æ–°é—»ï¼ˆå¦‚'Tesla latest news'ï¼‰
    """
    return search.run(query)
class WeatherInput(BaseModel):
    # Field é‡Œçš„ description éå¸¸é‡è¦ï¼Œå¤§æ¨¡å‹æ˜¯çœ‹è¿™ä¸ªæ¥ç†è§£å‚æ•°å«ä¹‰çš„
    city: str = Field(description="éœ€è¦æŸ¥è¯¢å¤©æ°”çš„åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šæ­å·ã€åŒ—äº¬ã€ä¸Šæµ·")

@tool("weather_tool", args_schema=WeatherInput)
def weather_tool(city: str):
    """è·å–æŒ‡å®šåŸå¸‚çš„å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚è¾“å…¥å‚æ•°ä¸ºåŸå¸‚åç§°ï¼ˆå¦‚'æ­å·'ï¼‰ã€‚"""
    print(f"ğŸ•µï¸ [Agent Action] æ­£åœ¨è°ƒç”¨é«˜å¾·APIæŸ¥è¯¢: {city}")
    return get_weather(city)

@tool
def time_tool():
    """è·å–å½“å‰ç³»ç»Ÿæ—¶é—´ã€‚æ— éœ€è¾“å…¥å‚æ•°ã€‚"""
    return get_current_time()

tools = [weather_tool, time_tool, search_tool,stock_tool]
tool_names = [t.name for t in tools]

# åˆå§‹åŒ–LLM

template = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åå°”è¡—é‡‘èåˆ†æå¸ˆã€‚ä½ çš„ç›®æ ‡æ˜¯åˆ©ç”¨å·¥å…·å›ç­”ç”¨æˆ·çš„é‡‘èé—®é¢˜ã€‚

ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·å¦‚ä¸‹ï¼š
{tools}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œæ€è€ƒå’Œå›ç­”ï¼ˆä¸è¦é—æ¼ä»»ä½•æ­¥éª¤ï¼‰ï¼š

Question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
Thought: æˆ‘åº”è¯¥æ€è€ƒæ¥ä¸‹æ¥åšä»€ä¹ˆ
Action: å·¥å…·åç§°ï¼Œå¿…é¡»æ˜¯ [{tool_names}] ä¸­çš„ä¸€ä¸ª
Action Input: å·¥å…·çš„è¾“å…¥å‚æ•°
Observation: å·¥å…·è¿”å›çš„ç»“æœ
... (Thought/Action/Action Input/Observation è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é‡å¤å¤šæ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: å¯¹åŸå§‹é—®é¢˜çš„æœ€ç»ˆå›ç­”ï¼ˆè¯·ä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«ã€æ•°æ®æ¦‚è§ˆã€‘ã€åˆ†æã€‘ã€å»ºè®®ã€‘ï¼‰

å¼€å§‹ï¼

Question: {input}
Thought:{agent_scratchpad}
"""

prompt = PromptTemplate.from_template(template)
# 1. æŠŠå·¥å…·åˆ—è¡¨è½¬æ¢æˆå­—ç¬¦ä¸²æè¿° (ä¾‹å¦‚: "weather: è·å–å¤©æ°”...")
#tools_text = render_text_description(tools)

# 2. æŠŠå·¥å…·åå­—åˆ—è¡¨è½¬æ¢æˆé€—å·åˆ†éš”çš„å­—ç¬¦ä¸² (ä¾‹å¦‚: "weather, search")
#tool_names_str = ", ".join([t.name for t in tools])

# 3. å¡«å……æ¨¡æ¿
#prompt = PromptTemplate.from_template(template).partial(
#    tools=tools_text,           # ä¼ å…¥å­—ç¬¦ä¸²
#    tool_names=tool_names_str,  # ä¼ å…¥å­—ç¬¦ä¸²
#)

llm = ChatOpenAI(
    model="Qwen/Qwen3-8B",
    temperature=0.1,
    streaming=True
)
# åˆ›å»ºæ™ºèƒ½ä½“REACT-Agent
agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=prompt,
    #stop_sequence=["\nObservation:"]
)

# åˆå§‹åŒ–Agentæ‰§è¡Œå™¨
agent_executor=AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)
# è®°å¿†ç®¡ç†
store = {}


def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


agent_with_history = RunnableWithMessageHistory(
    agent_executor,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)


# æµå¼ç”Ÿæˆå™¨
async def get_stream_response(query: str, session_id: str) -> AsyncIterable[str]:
    """
    æµå¼è¾“å‡ºã€‚
    æ³¨æ„ï¼šReAct Agent çš„è¾“å‡ºæ¯”è¾ƒå¤æ‚ã€‚
    å¦‚æœæƒ³è®©ç”¨æˆ·çœ‹åˆ° 'Thought' (æ€è€ƒè¿‡ç¨‹)ï¼Œå¯ä»¥æ”¾å®½è¿‡æ»¤æ¡ä»¶ã€‚
    å¦‚æœåªæƒ³è®©ç”¨æˆ·çœ‹åˆ° 'Final Answer'ï¼Œåˆ™åªè¿‡æ»¤ Final Answerã€‚
    """
    try:
        async for event in agent_with_history.astream_events(
                {"input": query},
                config={"configurable": {"session_id": session_id}},
                version="v1",
        ):
            kind = event["event"]

            # 1. æ•è· LLM çš„è¾“å‡ºæµ
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    # è¿™é‡Œçš„ content åŒ…å«äº† Thought, Action, Final Answer æ‰€æœ‰å†…å®¹
                    # ç›´æ¥ yield å‡ºå»ï¼Œç”¨æˆ·å°±èƒ½çœ‹åˆ° AI "ä¸€è¾¹æ€è€ƒä¸€è¾¹æ‰“å­—" çš„æ•ˆæœ
                    yield content

    except Exception as e:
        yield f"å‘ç”Ÿé”™è¯¯: {str(e)}"